#!/bin/bash

# Script de d√©marrage optimis√© pour l'IA locale avec Ollama
# Usage: ./start-local.sh

echo "üé• YouTube Video Scanner - Mode IA Locale"
echo "=========================================="

# V√©rifier que Docker est install√©
if ! command -v docker &> /dev/null; then
    echo "‚ùå Docker n'est pas install√©. Veuillez installer Docker d'abord."
    echo "   https://docs.docker.com/get-docker/"
    exit 1
fi

# V√©rifier que Docker Compose est install√©
if ! command -v docker-compose &> /dev/null; then
    echo "‚ùå Docker Compose n'est pas install√©. Veuillez installer Docker Compose d'abord."
    echo "   https://docs.docker.com/compose/install/"
    exit 1
fi

# Cr√©er les r√©pertoires de donn√©es
echo "üìÅ Cr√©ation des r√©pertoires de donn√©es..."
mkdir -p data/videos data/frames data/cache data/scanner_data data/ollama logs

# V√©rifier les ressources syst√®me
echo "üîç V√©rification des ressources syst√®me..."
TOTAL_RAM=$(free -g | awk '/^Mem:/{print $2}')
if [ "$TOTAL_RAM" -lt 4 ]; then
    echo "‚ö†Ô∏è  Attention: Moins de 4GB de RAM d√©tect√©s. L'IA locale peut √™tre lente."
    echo "   Recommand√©: Au moins 4GB de RAM pour Ollama + mod√®les de vision"
fi

# Construire et d√©marrer avec la configuration locale
echo "üî® Construction et d√©marrage avec IA locale (Ollama)..."
echo "   Cela peut prendre plusieurs minutes au premier d√©marrage..."
echo "   Les mod√®les IA seront t√©l√©charg√©s automatiquement."

docker-compose -f docker-compose.local.yml up -d --build

# Attendre que l'application soit pr√™te
echo "‚è≥ Attente du d√©marrage des services..."
echo "   - Ollama (IA locale)"
echo "   - Flask (API + Interface web)"

# Attendre plus longtemps pour le premier d√©marrage
sleep 30

# V√©rifier que l'application fonctionne
echo "üîç V√©rification des services..."

# V√©rifier Flask
if curl -s http://localhost:5000/api/scanner/health > /dev/null; then
    echo "‚úÖ Application Flask d√©marr√©e avec succ√®s!"
else
    echo "‚ö†Ô∏è  Application Flask en cours de d√©marrage..."
fi

# V√©rifier Ollama
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "‚úÖ Ollama (IA locale) d√©marr√© avec succ√®s!"
    
    # Afficher les mod√®les disponibles
    echo "üìã Mod√®les IA disponibles:"
    curl -s http://localhost:11434/api/tags | python3 -c "
import sys, json
try:
    data = json.load(sys.stdin)
    models = data.get('models', [])
    if models:
        for model in models:
            print(f'   - {model[\"name\"]}')
    else:
        print('   Aucun mod√®le install√© encore (t√©l√©chargement en cours...)')
except:
    print('   V√©rification des mod√®les en cours...')
"
else
    echo "‚ö†Ô∏è  Ollama en cours de d√©marrage..."
fi

echo ""
echo "üåê Acc√®s √† l'application:"
echo "   Interface web : http://localhost:5000"
echo "   API REST      : http://localhost:5000/api/scanner/"
echo "   Ollama API    : http://localhost:11434"
echo ""
echo "üìã Commandes utiles :"
echo "   docker-compose -f docker-compose.local.yml logs -f    # Voir les logs"
echo "   docker-compose -f docker-compose.local.yml down       # Arr√™ter"
echo "   docker-compose -f docker-compose.local.yml restart    # Red√©marrer"
echo ""
echo "ü§ñ D√©tecteurs IA prioritaires :"
echo "   1. Ollama (llava, moondream) - Vision IA locale"
echo "   2. YOLO local - D√©tection d'objets rapide"
echo "   3. OpenCV - D√©tection classique"
echo ""
echo "üéØ L'application privil√©gie automatiquement les processus locaux!"

# Optionnel: D√©marrer l'interface Ollama WebUI
read -p "üñ•Ô∏è  Voulez-vous aussi d√©marrer l'interface web Ollama ? [y/N] " -n 1 -r
echo
if [[ $REPLY =~ ^[Yy]$ ]]; then
    echo "üöÄ D√©marrage de l'interface Ollama WebUI..."
    docker-compose -f docker-compose.local.yml --profile webui up -d
    echo "‚úÖ Interface Ollama disponible sur: http://localhost:3000"
fi

