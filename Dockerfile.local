# Dockerfile optimisé pour les processus locaux avec Ollama
FROM python:3.11-slim

# Installer les dépendances système
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libgl1-mesa-glx \
    libglib2.0-0 \
    libsm6 \
    libxext6 \
    libxrender-dev \
    libgomp1 \
    curl \
    wget \
    && rm -rf /var/lib/apt/lists/*

# Installer Ollama
RUN curl -fsSL https://ollama.ai/install.sh | sh

# Créer le répertoire de travail
WORKDIR /app

# Copier les fichiers de requirements
COPY requirements.txt .

# Installer les dépendances Python
RUN pip install --no-cache-dir -r requirements.txt

# Copier le code source
COPY src/ ./src/
COPY *.py ./

# Créer les répertoires nécessaires
RUN mkdir -p /app/data/videos /app/data/frames /app/data/cache /app/data/scanner_data /app/data/ollama

# Exposer les ports (Flask + Ollama)
EXPOSE 5000 11434

# Variables d'environnement
ENV FLASK_APP=src/main.py
ENV FLASK_ENV=production
ENV PYTHONPATH=/app
ENV OLLAMA_HOST=0.0.0.0:11434
ENV OLLAMA_MODELS=/app/data/ollama

# Script de démarrage qui lance Ollama puis Flask
COPY start-services.sh /app/
RUN chmod +x /app/start-services.sh

# Commande de démarrage
CMD ["/app/start-services.sh"]

